{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nvidia_ssd_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14d360ea40f846e089f4513ea1ff1da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1517445bc673492eaec4777ce4c58ea2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_02e8cf37f50744b1abc6b916a569f381",
              "IPY_MODEL_6d30cc066a6b47ccb70a601c6fe892fa"
            ]
          }
        },
        "1517445bc673492eaec4777ce4c58ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02e8cf37f50744b1abc6b916a569f381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5e3e746e071f42b8a1de6fbc3a49083b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51b82f348aee4b75bf494ce065493eae"
          }
        },
        "6d30cc066a6b47ccb70a601c6fe892fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae710e8e55a34d1c8e2fb822ffbca490",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:14&lt;00:00, 6.91MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b720c9ae15645e982f3ce8bd8edd205"
          }
        },
        "5e3e746e071f42b8a1de6fbc3a49083b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51b82f348aee4b75bf494ce065493eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae710e8e55a34d1c8e2fb822ffbca490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b720c9ae15645e982f3ce8bd8edd205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergeevgithub/deep_learning_2018-19/blob/master/second/17.2%20nvidia_ssd_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNKDRrsQiM9_"
      },
      "source": [
        "# Neural Object Detection: Practice\n",
        "## Part 1: SSD inference \n",
        "\n",
        "*Ilya Zakharkin, November 2020*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91JWyzIriXpq"
      },
      "source": [
        "---\n",
        "### This notebook requires a GPU runtime to run.\n",
        "### Please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJilyHp-kHwI"
      },
      "source": [
        "Previous year there was a [good Computer Vision practice lesson](https://www.youtube.com/watch?v=XSPYe4-y4HE) (RUS), its [notebooks](https://drive.google.com/drive/folders/1ZpS8oyI__3QvjoIwVZwur4I5e4qIExPS?usp=sharing).\n",
        "\n",
        "This year we will master inference and training of the SSD neural network discussed on the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArqmTCyxrNnV"
      },
      "source": [
        "This notebook is based on [NVIDIA deep learning examples](https://github.com/NVIDIA/DeepLearningExamples ). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd8yjlbevxdO"
      },
      "source": [
        "**[Cinematic NVIDIA keynotes](https://www.youtube.com/watch?v=CKnipnFsuFo&list=PLZHnYvH1qtOYOfzAj7JZFwqtabM5XPku1) on Deep Learning 2020.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HX1V2YOcvBS"
      },
      "source": [
        "\n",
        "\n",
        "# SSD\n",
        "\n",
        "*Author: NVIDIA*\n",
        "\n",
        "**Single Shot MultiBox Detector model for object detection**\n",
        "\n",
        "_ | _\n",
        "- | -\n",
        "![alt](https://pytorch.org/assets/images/ssd_diagram.png) | ![alt](https://pytorch.org/assets/images/ssd.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lOu1NkecvBS",
        "outputId": "0a138c39-6097-49b9-a772-317ea67bf41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "14d360ea40f846e089f4513ea1ff1da0",
            "1517445bc673492eaec4777ce4c58ea2",
            "02e8cf37f50744b1abc6b916a569f381",
            "6d30cc066a6b47ccb70a601c6fe892fa",
            "5e3e746e071f42b8a1de6fbc3a49083b",
            "51b82f348aee4b75bf494ce065493eae",
            "ae710e8e55a34d1c8e2fb822ffbca490",
            "4b720c9ae15645e982f3ce8bd8edd205"
          ]
        }
      },
      "source": [
        "import torch\n",
        "precision = 'fp32'\n",
        "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math=precision)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/archive/torchhub.zip\" to /root/.cache/torch/hub/torchhub.zip\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d360ea40f846e089f4513ea1ff1da0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/ssd_pyt_ckpt_amp/versions/19.09.0/files/nvidia_ssdpyt_fp16_190826.pt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHgkvxllcvBS"
      },
      "source": [
        "will load an SSD model pretrained on COCO dataset from Torch Hub.\n",
        "\n",
        "Setting precision='fp16' will load a checkpoint trained with [mixed precision](https://arxiv.org/abs/1710.03740) into architecture enabling execution on [Tensor Cores](https://developer.nvidia.com/tensor-cores).\n",
        "Handling mixed precision data requires [Apex](https://github.com/NVIDIA/apex) library.\n",
        "\n",
        "\n",
        "\n",
        "### Model Description\n",
        "\n",
        "This SSD300 model is based on the\n",
        "[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) paper, which\n",
        "describes SSD as “a method for detecting objects in images using a single deep neural network\".\n",
        "The input size is fixed to 300x300.\n",
        "\n",
        "The main difference between this model and the one described in the paper is in the backbone.\n",
        "Specifically, the VGG model is obsolete and is replaced by the ResNet-50 model.\n",
        "\n",
        "From the\n",
        "[Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/abs/1611.10012)\n",
        "paper, the following enhancements were made to the backbone:\n",
        "*   The conv5_x, avgpool, fc and softmax layers were removed from the original classification model.\n",
        "*   All strides in conv4_x are set to 1x1.\n",
        "\n",
        "The backbone is followed by 5 additional convolutional layers.\n",
        "In addition to the convolutional layers, we attached 6 detection heads:\n",
        "*   The first detection head is attached to the last conv4_x layer.\n",
        "*   The other five detection heads are attached to the corresponding 5 additional layers.\n",
        "\n",
        "Detector heads are similar to the ones referenced in the paper, however,\n",
        "they are enhanced by additional BatchNorm layers after each convolution.\n",
        "\n",
        "### Example\n",
        "\n",
        "In the example below we will use the pretrained SSD model loaded from Torch Hub to detect objects in sample images and visualize the result.\n",
        "\n",
        "To run the example you need some extra python packages installed.\n",
        "These are needed for preprocessing images and visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwXIolUccvBS",
        "outputId": "7597860d-865b-440c-e108-372a1d49fbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash\n",
        "pip install numpy scipy scikit-image matplotlib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwRDdp3McvBS"
      },
      "source": [
        "For convenient and comprehensive formatting of input and output of the model, load a set of utility methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMTLeiern1xB",
        "outputId": "788ac4e4-f486-4a33-bd20-438d178f81f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# List of available models in PyTorch Hub from Nvidia/DeepLearningExamples\n",
        "torch.hub.list('NVIDIA/DeepLearningExamples:torchhub')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['checkpoint_from_distributed',\n",
              " 'nvidia_ncf',\n",
              " 'nvidia_ssd',\n",
              " 'nvidia_ssd_processing_utils',\n",
              " 'nvidia_tacotron2',\n",
              " 'nvidia_waveglow',\n",
              " 'unwrap_distributed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCaFI1DdcvBT",
        "outputId": "0df6a4c3-f0da-4d5c-e8db-a44d23d441f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWeB1LylyYy"
      },
      "source": [
        "File that has contains all this code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgcS6QJBf0lw",
        "outputId": "89ace2bf-19aa-458f-edcf-7c5bcff711a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/hubconf.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import urllib.request\n",
            "import torch\n",
            "import os\n",
            "import sys\n",
            "\n",
            "\n",
            "# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py\n",
            "def checkpoint_from_distributed(state_dict):\n",
            "    \"\"\"\n",
            "    Checks whether checkpoint was generated by DistributedDataParallel. DDP\n",
            "    wraps model in additional \"module.\", it needs to be unwrapped for single\n",
            "    GPU inference.\n",
            "    :param state_dict: model's state dict\n",
            "    \"\"\"\n",
            "    ret = False\n",
            "    for key, _ in state_dict.items():\n",
            "        if key.find('module.') != -1:\n",
            "            ret = True\n",
            "            break\n",
            "    return ret\n",
            "\n",
            "\n",
            "# from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/inference.py\n",
            "def unwrap_distributed(state_dict):\n",
            "    \"\"\"\n",
            "    Unwraps model from DistributedDataParallel.\n",
            "    DDP wraps model in additional \"module.\", it needs to be removed for single\n",
            "    GPU inference.\n",
            "    :param state_dict: model's state dict\n",
            "    \"\"\"\n",
            "    new_state_dict = {}\n",
            "    for key, value in state_dict.items():\n",
            "        new_key = key.replace('module.1.', '')\n",
            "        new_key = new_key.replace('module.', '')\n",
            "        new_state_dict[new_key] = value\n",
            "    return new_state_dict\n",
            "\n",
            "\n",
            "def _download_checkpoint(checkpoint, force_reload):\n",
            "    model_dir = os.path.join(torch.hub._get_torch_home(), 'checkpoints')\n",
            "    if not os.path.exists(model_dir):\n",
            "        os.makedirs(model_dir)\n",
            "    ckpt_file = os.path.join(model_dir, os.path.basename(checkpoint))\n",
            "    if not os.path.exists(ckpt_file) or force_reload:\n",
            "        sys.stderr.write('Downloading checkpoint from {}\\n'.format(checkpoint))\n",
            "        urllib.request.urlretrieve(checkpoint, ckpt_file)\n",
            "    return ckpt_file\n",
            "\n",
            "\n",
            "dependencies = ['torch']\n",
            "\n",
            "\n",
            "def nvidia_ncf(pretrained=True, **kwargs):\n",
            "    \"\"\"Constructs an NCF model.\n",
            "    For detailed information on model input and output, training recipies, inference and performance\n",
            "    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com\n",
            "\n",
            "    Args:\n",
            "        pretrained (bool, True): If True, returns a model pretrained on ml-20m dataset.\n",
            "        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')\n",
            "        nb_users (int): number of users\n",
            "        nb_items (int): number of items\n",
            "        mf_dim (int, 64): dimension of latent space in matrix factorization\n",
            "        mlp_layer_sizes (list, [256,256,128,64]): sizes of layers of multi-layer-perceptron\n",
            "        dropout (float, 0.5): dropout\n",
            "    \"\"\"\n",
            "\n",
            "    from PyTorch.Recommendation.NCF import neumf as ncf\n",
            "\n",
            "    fp16 = \"model_math\" in kwargs and kwargs[\"model_math\"] == \"fp16\"\n",
            "    force_reload = \"force_reload\" in kwargs and kwargs[\"force_reload\"]\n",
            "\n",
            "    config = {'nb_users': None, 'nb_items': None, 'mf_dim': 64, 'mf_reg': 0.,\n",
            "              'mlp_layer_sizes': [256, 256, 128, 64], 'mlp_layer_regs':[0, 0, 0, 0], 'dropout': 0.5}\n",
            "\n",
            "    if pretrained:\n",
            "        if fp16:\n",
            "            checkpoint = 'https://developer.nvidia.com/joc-ncf-fp16-pyt-20190225'\n",
            "        else:\n",
            "            checkpoint = 'https://developer.nvidia.com/joc-ncf-fp32-pyt-20190225'\n",
            "        ckpt_file = _download_checkpoint(checkpoint, force_reload)\n",
            "        ckpt = torch.load(ckpt_file)\n",
            "\n",
            "        if checkpoint_from_distributed(ckpt):\n",
            "            ckpt = unwrap_distributed(ckpt)\n",
            "\n",
            "        config['nb_users'] = ckpt['mf_user_embed.weight'].shape[0]\n",
            "        config['nb_items'] = ckpt['mf_item_embed.weight'].shape[0]\n",
            "        config['mf_dim'] = ckpt['mf_item_embed.weight'].shape[1]\n",
            "        mlp_shapes = [ckpt[k].shape for k in ckpt.keys() if 'mlp' in k and 'weight' in k and 'embed' not in k]\n",
            "        config['mlp_layer_sizes'] = [mlp_shapes[0][1], mlp_shapes[1][1], mlp_shapes[2][1],  mlp_shapes[2][0]]\n",
            "        config['mlp_layer_regs'] = [0] * len(config['mlp_layer_sizes'])\n",
            "\n",
            "    else:\n",
            "        if 'nb_users' not in kwargs:\n",
            "            raise ValueError(\"Missing 'nb_users' argument.\")\n",
            "        if 'nb_items' not in kwargs:\n",
            "            raise ValueError(\"Missing 'nb_items' argument.\")\n",
            "        for k,v in kwargs.items():\n",
            "            if k in config.keys():\n",
            "                config[k] = v\n",
            "        config['mlp_layer_regs'] = [0] * len(config['mlp_layer_sizes'])\n",
            "\n",
            "    m = ncf.NeuMF(**config)\n",
            "\n",
            "    if fp16:\n",
            "        m.half()\n",
            "\n",
            "    if pretrained:\n",
            "        m.load_state_dict(ckpt)\n",
            "\n",
            "    return m\n",
            "\n",
            "\n",
            "def nvidia_tacotron2(pretrained=True, **kwargs):\n",
            "    \"\"\"Constructs a Tacotron 2 model (nn.module with additional infer(input) method).\n",
            "    For detailed information on model input and output, training recipies, inference and performance\n",
            "    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com\n",
            "\n",
            "    Args (type[, default value]):\n",
            "        pretrained (bool, True): If True, returns a model pretrained on LJ Speech dataset.\n",
            "        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')\n",
            "        n_symbols (int, 148): Number of symbols used in a sequence passed to the prenet, see\n",
            "                              https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechSynthesis/Tacotron2/tacotron2/text/symbols.py\n",
            "        p_attention_dropout (float, 0.1): dropout probability on attention LSTM (1st LSTM layer in decoder)\n",
            "        p_decoder_dropout (float, 0.1): dropout probability on decoder LSTM (2nd LSTM layer in decoder)\n",
            "        max_decoder_steps (int, 1000): maximum number of generated mel spectrograms during inference\n",
            "    \"\"\"\n",
            "\n",
            "    from PyTorch.SpeechSynthesis.Tacotron2.tacotron2 import model as tacotron2\n",
            "    from PyTorch.SpeechSynthesis.Tacotron2.models import lstmcell_to_float, batchnorm_to_float\n",
            "    from PyTorch.SpeechSynthesis.Tacotron2.tacotron2.text import text_to_sequence\n",
            "\n",
            "    fp16 = \"model_math\" in kwargs and kwargs[\"model_math\"] == \"fp16\"\n",
            "    force_reload = \"force_reload\" in kwargs and kwargs[\"force_reload\"]\n",
            "\n",
            "    if pretrained:\n",
            "        if fp16:\n",
            "            checkpoint = 'https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427'\n",
            "        else:\n",
            "            checkpoint = 'https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_fp32/versions/19.09.0/files/nvidia_tacotron2pyt_fp32_20190427'\n",
            "        ckpt_file = _download_checkpoint(checkpoint, force_reload)\n",
            "        ckpt = torch.load(ckpt_file)\n",
            "        state_dict = ckpt['state_dict']\n",
            "        if checkpoint_from_distributed(state_dict):\n",
            "            state_dict = unwrap_distributed(state_dict)\n",
            "        config = ckpt['config']\n",
            "    else:\n",
            "        config = {'mask_padding': False, 'n_mel_channels': 80, 'n_symbols': 148,\n",
            "                  'symbols_embedding_dim': 512, 'encoder_kernel_size': 5,\n",
            "                  'encoder_n_convolutions': 3, 'encoder_embedding_dim': 512,\n",
            "                  'attention_rnn_dim': 1024, 'attention_dim': 128,\n",
            "                  'attention_location_n_filters': 32,\n",
            "                  'attention_location_kernel_size': 31, 'n_frames_per_step': 1,\n",
            "                  'decoder_rnn_dim': 1024, 'prenet_dim': 256,\n",
            "                  'max_decoder_steps': 1000, 'gate_threshold': 0.5,\n",
            "                  'p_attention_dropout': 0.1, 'p_decoder_dropout': 0.1,\n",
            "                  'postnet_embedding_dim': 512, 'postnet_kernel_size': 5,\n",
            "                  'postnet_n_convolutions': 5, 'decoder_no_early_stopping': False}\n",
            "        for k,v in kwargs.items():\n",
            "            if k in config.keys():\n",
            "                config[k] = v\n",
            "\n",
            "    m = tacotron2.Tacotron2(**config)\n",
            "\n",
            "    if fp16:\n",
            "        m = batchnorm_to_float(m.half())\n",
            "        m = lstmcell_to_float(m)\n",
            "\n",
            "    if pretrained:\n",
            "        m.load_state_dict(state_dict)\n",
            "\n",
            "    m.text_to_sequence = text_to_sequence\n",
            "\n",
            "    return m\n",
            "\n",
            "\n",
            "def nvidia_waveglow(pretrained=True, **kwargs):\n",
            "    \"\"\"Constructs a WaveGlow model (nn.module with additional infer(input) method).\n",
            "    For detailed information on model input and output, training recipies, inference and performance\n",
            "    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com\n",
            "\n",
            "    Args:\n",
            "        pretrained (bool): If True, returns a model pretrained on LJ Speech dataset.\n",
            "        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')\n",
            "    \"\"\"\n",
            "\n",
            "    from PyTorch.SpeechSynthesis.Tacotron2.waveglow import model as waveglow\n",
            "    from PyTorch.SpeechSynthesis.Tacotron2.models import batchnorm_to_float\n",
            "\n",
            "    fp16 = \"model_math\" in kwargs and kwargs[\"model_math\"] == \"fp16\"\n",
            "    force_reload = \"force_reload\" in kwargs and kwargs[\"force_reload\"]\n",
            "\n",
            "    if pretrained:\n",
            "        if fp16:\n",
            "            checkpoint = 'https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427'\n",
            "        else:\n",
            "            checkpoint = 'https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427'\n",
            "        ckpt_file = _download_checkpoint(checkpoint, force_reload)\n",
            "        ckpt = torch.load(ckpt_file)\n",
            "        state_dict = ckpt['state_dict']\n",
            "        if checkpoint_from_distributed(state_dict):\n",
            "            state_dict = unwrap_distributed(state_dict)\n",
            "        config = ckpt['config']\n",
            "    else:\n",
            "        config = {'n_mel_channels': 80, 'n_flows': 12, 'n_group': 8,\n",
            "                  'n_early_every': 4, 'n_early_size': 2,\n",
            "                  'WN_config': {'n_layers': 8, 'kernel_size': 3,\n",
            "                                'n_channels': 512}}\n",
            "        for k,v in kwargs.items():\n",
            "            if k in config.keys():\n",
            "                config[k] = v\n",
            "            elif k in config['WN_config'].keys():\n",
            "                config['WN_config'][k] = v\n",
            "\n",
            "    m = waveglow.WaveGlow(**config)\n",
            "\n",
            "    if fp16:\n",
            "        m = batchnorm_to_float(m.half())\n",
            "        for mat in m.convinv:\n",
            "            mat.float()\n",
            "\n",
            "    if pretrained:\n",
            "        m.load_state_dict(state_dict)\n",
            "\n",
            "    return m\n",
            "\n",
            "def nvidia_ssd_processing_utils():\n",
            "    import numpy as np\n",
            "    import skimage\n",
            "    from skimage import io, transform\n",
            "\n",
            "    from PyTorch.Detection.SSD.src.utils import dboxes300_coco, Encoder\n",
            "\n",
            "    class Processing:\n",
            "        @staticmethod\n",
            "        def load_image(image_path):\n",
            "            \"\"\"Code from Loading_Pretrained_Models.ipynb - a Caffe2 tutorial\"\"\"\n",
            "            img = skimage.img_as_float(io.imread(image_path))\n",
            "            if len(img.shape) == 2:\n",
            "                img = np.array([img, img, img]).swapaxes(0, 2)\n",
            "            return img\n",
            "\n",
            "        @staticmethod\n",
            "        def rescale(img, input_height, input_width):\n",
            "            \"\"\"Code from Loading_Pretrained_Models.ipynb - a Caffe2 tutorial\"\"\"\n",
            "            aspect = img.shape[1] / float(img.shape[0])\n",
            "            if (aspect > 1):\n",
            "                # landscape orientation - wide image\n",
            "                res = int(aspect * input_height)\n",
            "                imgScaled = transform.resize(img, (input_width, res))\n",
            "            if (aspect < 1):\n",
            "                # portrait orientation - tall image\n",
            "                res = int(input_width / aspect)\n",
            "                imgScaled = transform.resize(img, (res, input_height))\n",
            "            if (aspect == 1):\n",
            "                imgScaled = transform.resize(img, (input_width, input_height))\n",
            "            return imgScaled\n",
            "\n",
            "        @staticmethod\n",
            "        def crop_center(img, cropx, cropy):\n",
            "            \"\"\"Code from Loading_Pretrained_Models.ipynb - a Caffe2 tutorial\"\"\"\n",
            "            y, x, c = img.shape\n",
            "            startx = x // 2 - (cropx // 2)\n",
            "            starty = y // 2 - (cropy // 2)\n",
            "            return img[starty:starty + cropy, startx:startx + cropx]\n",
            "\n",
            "        @staticmethod\n",
            "        def normalize(img, mean=128, std=128):\n",
            "            img = (img * 256 - mean) / std\n",
            "            return img\n",
            "\n",
            "        @staticmethod\n",
            "        def prepare_tensor(inputs, fp16=False):\n",
            "            NHWC = np.array(inputs)\n",
            "            NCHW = np.swapaxes(np.swapaxes(NHWC, 1, 3), 2, 3)\n",
            "            tensor = torch.from_numpy(NCHW)\n",
            "            tensor = tensor.contiguous()\n",
            "            tensor = tensor.cuda()\n",
            "            tensor = tensor.float()\n",
            "            if fp16:\n",
            "                tensor = tensor.half()\n",
            "            return tensor\n",
            "\n",
            "        @staticmethod\n",
            "        def prepare_input(img_uri):\n",
            "            img = Processing.load_image(img_uri)\n",
            "            img = Processing.rescale(img, 300, 300)\n",
            "            img = Processing.crop_center(img, 300, 300)\n",
            "            img = Processing.normalize(img)\n",
            "            return img\n",
            "\n",
            "        @staticmethod\n",
            "        def decode_results(predictions):\n",
            "            dboxes = dboxes300_coco()\n",
            "            encoder = Encoder(dboxes)\n",
            "            ploc, plabel = [val.float() for val in predictions]\n",
            "            results = encoder.decode_batch(ploc, plabel, criteria=0.5, max_output=20)\n",
            "            return [[pred.detach().cpu().numpy() for pred in detections] for detections in results]\n",
            "\n",
            "        @staticmethod\n",
            "        def pick_best(detections, threshold=0.3):\n",
            "            bboxes, classes, confidences = detections\n",
            "            best = np.argwhere(confidences > threshold)[:, 0]\n",
            "            return [pred[best] for pred in detections]\n",
            "\n",
            "        @staticmethod\n",
            "        def get_coco_object_dictionary():\n",
            "            import os\n",
            "            file_with_coco_names = \"category_names.txt\"\n",
            "\n",
            "            if not os.path.exists(file_with_coco_names):\n",
            "                print(\"Downloading COCO annotations.\")\n",
            "                import urllib\n",
            "                import zipfile\n",
            "                import json\n",
            "                import shutil\n",
            "                urllib.request.urlretrieve(\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\", \"cocoanno.zip\")\n",
            "                with zipfile.ZipFile(\"cocoanno.zip\", \"r\") as f:\n",
            "                    f.extractall()\n",
            "                print(\"Downloading finished.\")\n",
            "                with open(\"annotations/instances_val2017.json\", 'r') as COCO:\n",
            "                    js = json.loads(COCO.read())\n",
            "                class_names = [category['name'] for category in js['categories']]\n",
            "                open(\"category_names.txt\", 'w').writelines([c+\"\\n\" for c in class_names])\n",
            "                os.remove(\"cocoanno.zip\")\n",
            "                shutil.rmtree(\"annotations\")\n",
            "            else:\n",
            "                class_names = open(\"category_names.txt\").readlines()\n",
            "                class_names = [c.strip() for c in class_names]\n",
            "            return class_names\n",
            "\n",
            "    return Processing()\n",
            "\n",
            "\n",
            "def nvidia_ssd(pretrained=True, **kwargs):\n",
            "    \"\"\"Constructs an SSD300 model.\n",
            "    For detailed information on model input and output, training recipies, inference and performance\n",
            "    visit: github.com/NVIDIA/DeepLearningExamples and/or ngc.nvidia.com\n",
            "\n",
            "    Args:\n",
            "        pretrained (bool, True): If True, returns a model pretrained on COCO dataset.\n",
            "        model_math (str, 'fp32'): returns a model in given precision ('fp32' or 'fp16')\n",
            "    \"\"\"\n",
            "\n",
            "    from PyTorch.Detection.SSD.src import model as ssd\n",
            "\n",
            "    fp16 = \"model_math\" in kwargs and kwargs[\"model_math\"] == \"fp16\"\n",
            "    force_reload = \"force_reload\" in kwargs and kwargs[\"force_reload\"]\n",
            "\n",
            "    m = ssd.SSD300()\n",
            "    if fp16:\n",
            "        m = m.half()\n",
            "\n",
            "        def batchnorm_to_float(module):\n",
            "            \"\"\"Converts batch norm to FP32\"\"\"\n",
            "            if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
            "                module.float()\n",
            "            for child in module.children():\n",
            "                batchnorm_to_float(child)\n",
            "            return module\n",
            "\n",
            "        m = batchnorm_to_float(m)\n",
            "\n",
            "    if pretrained:\n",
            "        checkpoint = 'https://api.ngc.nvidia.com/v2/models/nvidia/ssd_pyt_ckpt_amp/versions/19.09.0/files/nvidia_ssdpyt_fp16_190826.pt'\n",
            "        # ckpt = torch.hub.load_state_dict_from_url(checkpoint, progress=True, check_hash=False)\n",
            "        ckpt_file = _download_checkpoint(checkpoint, force_reload)\n",
            "        ckpt = torch.load(ckpt_file)\n",
            "        ckpt = ckpt['model']\n",
            "        if checkpoint_from_distributed(ckpt):\n",
            "            ckpt = unwrap_distributed(ckpt)\n",
            "        m.load_state_dict(ckpt)\n",
            "    return m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVeTWqN7qC7g"
      },
      "source": [
        "File [utils.py](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Detection/SSD/src/utils.py) that containes Encoder and dboxes_300 code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrnr5BehcvBT"
      },
      "source": [
        "Now, prepare the loaded model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mV4jTiDcvBT",
        "outputId": "cda7c2ed-ae91-4c9d-835e-944c5060d767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ssd_model.to('cuda')\n",
        "ssd_model.eval()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SSD300(\n",
              "  (feature_extractor): ResNet(\n",
              "    (feature_extractor): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (additional_blocks): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (loc): ModuleList(\n",
              "    (0): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (conf): ModuleList(\n",
              "    (0): Conv2d(1024, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxL2kmlacvBT"
      },
      "source": [
        "Prepare input images for object detection.\n",
        "(Example links below correspond to first few test images from the COCO dataset, but you can also specify paths to your local images here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSW3vWh6cvBT"
      },
      "source": [
        "uris = [\n",
        "    'http://images.cocodataset.org/val2017/000000397133.jpg',\n",
        "    'http://images.cocodataset.org/val2017/000000037777.jpg',\n",
        "    'http://images.cocodataset.org/val2017/000000252219.jpg',\n",
        "    'https://i.ibb.co/ZVqGsbJ/nvidia-image-300.jpg'\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVn-X4r_cvBT"
      },
      "source": [
        "Format the images to comply with the network input and convert them to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOV1RlrpcvBT"
      },
      "source": [
        "inputs = [utils.prepare_input(uri) for uri in uris]\n",
        "tensor = utils.prepare_tensor(inputs, precision == 'fp16')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASSoSQzwcvBT"
      },
      "source": [
        "Run the SSD network to perform object detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXSCjU1-cvBT"
      },
      "source": [
        "with torch.no_grad():\n",
        "    detections_batch = ssd_model(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4hvNjBQd-Ur"
      },
      "source": [
        "print(len(detections_batch))\n",
        "print(len(detections_batch[0]), detections_batch[0].shape)\n",
        "print(len(detections_batch[1]), detections_batch[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W9GIaixcvBT"
      },
      "source": [
        "By default, raw output from SSD network per input image contains\n",
        "8732 boxes with localization and class probability distribution.\n",
        "Let's filter this output to only get reasonable detections (confidence>40%) in a more comprehensive format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNRfspJ-cvBT"
      },
      "source": [
        "results_per_input = utils.decode_results(detections_batch)\n",
        "print(len(results_per_input), len(results_per_input[0]), \n",
        "      results_per_input[1][0].shape, results_per_input[1][1].shape, results_per_input[1][2].shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7VG6ukerI5"
      },
      "source": [
        "best_results_per_input = [utils.pick_best(results, 0.20) for results in results_per_input]\n",
        "print(len(best_results_per_input), len(best_results_per_input[0]), \n",
        "      best_results_per_input[1][0].shape, best_results_per_input[1][1].shape, best_results_per_input[1][2].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91J0bPgcvBT"
      },
      "source": [
        "The model was trained on COCO dataset, which we need to access in order to translate class IDs into object names.\n",
        "For the first time, downloading annotations may take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_MgyPGhcvBT"
      },
      "source": [
        "classes_to_labels = utils.get_coco_object_dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x82pefYcvBT"
      },
      "source": [
        "Finally, let's visualize our detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMnyNTArcvBT"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "for image_idx in range(len(best_results_per_input)):\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Show original, denormalized image...\n",
        "    image = inputs[image_idx] / 2 + 0.5\n",
        "    ax.imshow(image)\n",
        "    # ...with detections\n",
        "    bboxes, classes, confidences = best_results_per_input[image_idx]\n",
        "    for idx in range(len(bboxes)):\n",
        "        left, bot, right, top = bboxes[idx]\n",
        "        x, y, w, h = [val * 300 for val in [left, bot, right - left, top - bot]]\n",
        "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y, \"{} {:.0f}%\".format(classes_to_labels[classes[idx] - 1], confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTntQAfnUvf"
      },
      "source": [
        "> To run this SSD implementation on the 4th image we needed:\n",
        "- find it in Google via \"Image search\";\n",
        "- manually resize it to 300х300;\n",
        "- lower the threshold down to 20% (originally it was 40%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F0gATP1pjH8"
      },
      "source": [
        "Benchmark utility :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nIS_Nc4pgeI"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# Helper function to benchmark the model\n",
        "def benchmark(model, input_shape=(1024, 1, 32, 32), dtype='fp32', nwarmup=50, nruns=1000):\n",
        "    input_data = torch.randn(input_shape)\n",
        "    input_data = input_data.to(\"cuda\")\n",
        "    if dtype=='fp16':\n",
        "        input_data = input_data.half()\n",
        "\n",
        "    print(\"Warm up ...\")\n",
        "    with torch.no_grad():\n",
        "        for _ in range(nwarmup):\n",
        "            features = model(input_data)\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Start timing ...\")\n",
        "    timings = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(1, nruns+1):\n",
        "            start_time = time.time()\n",
        "            pred_loc, pred_label  = model(input_data)\n",
        "            torch.cuda.synchronize()\n",
        "            end_time = time.time()\n",
        "            timings.append(end_time - start_time)\n",
        "            if i%100==0:\n",
        "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
        "\n",
        "    print(\"Input shape:\", input_data.size())\n",
        "    print(\"Output location prediction size:\", pred_loc.size())\n",
        "    print(\"Output label prediction size:\", pred_label.size())\n",
        "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDINqjo3pk8a"
      },
      "source": [
        "We check how well the model performs **before** we use TRTorch/TensorRT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34YoEAozplB0"
      },
      "source": [
        "# Model benchmark without TRTorch/TensorRT\n",
        "model = ssd_model.eval().to(\"cuda\")\n",
        "benchmark(model, input_shape=(128, 3, 300, 300), nruns=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXLW-xY5qy9b"
      },
      "source": [
        "After TensorRT acceleration: [follow this guide](https://nvidia.github.io/TRTorch/_notebooks/ssd-object-detection-demo.html) (scroll down the page)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGu0EiWNq8ne"
      },
      "source": [
        "...Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPjgnY6Gq_eZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCvXw7kXq_hT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MBAowA8cvBT"
      },
      "source": [
        "### Details\n",
        "For detailed information on model input and output,\n",
        "training recipies, inference and performance visit:\n",
        "[github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)\n",
        "and/or [NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:ssd_for_pytorch)\n",
        "\n",
        "### Useful links\n",
        "\n",
        " - [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) paper\n",
        " - [Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/abs/1611.10012) paper\n",
        " - [SSD on NGC](https://ngc.nvidia.com/catalog/model-scripts/nvidia:ssd_for_pytorch)\n",
        " - [SSD on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)\n",
        " - [SSD300 TensorRT tutorial](https://nvidia.github.io/TRTorch/_notebooks/ssd-object-detection-demo.html)"
      ]
    }
  ]
}